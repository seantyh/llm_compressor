{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if '../src' not in sys.path:\n",
    "  sys.path.append('../src')\n",
    "import os\n",
    "os.environ[\"BNB_CUDA_VERSION\"] = \"115\"\n",
    "import numpy as np\n",
    "from llm_compressor import AECompressorLLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompt_ids = tokenizer(\"This\", return_tensors=\"pt\").input_ids\n",
    "# gentext = tokenizer.batch_decode(\n",
    "#             model.generate(input_ids=prompt_ids, \n",
    "#             max_new_tokens=20, pad_token_id=tokenizer.eos_token_id))[0]\n",
    "# gentext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "test_text = \"The hypernym of cat is animal\"\n",
    "input_ids = tokenizer(test_text, return_tensors=\"pt\").input_ids\n",
    "# input_ids = input_ids[:, :-1]\n",
    "with torch.no_grad():\n",
    "  logits = model(input_ids).logits.squeeze()\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "uniform_prob = torch.ones(probs.shape[1]) / probs.shape[1]\n",
    "next_token_probs = torch.concat([uniform_prob.unsqueeze(0), probs[:-1, :]], dim=0)\n",
    "uniform_nt_probs = torch.ones_like(probs) / probs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1])\n",
      "tensor([1.9898e-05, 4.2627e-05, 1.1066e-05, 1.2249e-01, 3.8000e-02, 1.4243e-04,\n",
      "        1.6088e-03, 1.0009e-04])\n",
      "['The', 'Ġhyper', 'ny', 'm', 'Ġof', 'Ġcat', 'Ġis', 'Ġanimal']\n"
     ]
    }
   ],
   "source": [
    "# next_token_probs: (seq_len, vocab_size)\n",
    "# input_ids: (batch_size, seq_len)\n",
    "print(input_ids.squeeze().unsqueeze(1).shape)\n",
    "print(next_token_probs.gather(dim=1, index=input_ids.squeeze().unsqueeze(1)).squeeze())\n",
    "print(tokenizer.convert_ids_to_tokens(input_ids.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message length: 90 bits\n",
      "data length: 128 bits\n",
      "compress ratio: 0.7031\n"
     ]
    }
   ],
   "source": [
    "compressor = AECompressorLLM()\n",
    "data_ids = input_ids.squeeze().tolist()\n",
    "\n",
    "msg = compressor.compress(data_ids, next_token_probs)\n",
    "recon = compressor.decompress(msg, len(data_ids), next_token_probs)\n",
    "## uniform probs baseline\n",
    "# msg = compressor.compress(data_ids, uniform_nt_probs)\n",
    "# recon = compressor.decompress(msg, len(data_ids), uniform_nt_probs)\n",
    "\n",
    "assert all(a==b for a, b in zip(recon, data_ids))\n",
    "msg_len = len(msg)\n",
    "data_len = len(data_ids) * 16\n",
    "print(f\"message length: {msg_len} bits\")\n",
    "print(f\"data length: {data_len} bits\")\n",
    "print(f\"compress ratio: {msg_len/data_len:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib\n",
    "zmsg = zlib.compress(test_text.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2758620689655173"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zmsg)/len(test_text.encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clear float2repr implmentation by Copilot\n",
    "import struct\n",
    "\n",
    "def float_repr(num):\n",
    "    # pack the float into a bytes object\n",
    "    packed = struct.pack('f', num)\n",
    "    \n",
    "    # unpack the bytes object to get the exponent and fractional part\n",
    "    bits = struct.unpack('I', packed)[0]    \n",
    "    sign = bits >> 31\n",
    "    exp = (bits >> 23) & 0xff\n",
    "    frac = bits & 0x7fffff\n",
    "    \n",
    "    # convert the exponent to a signed integer\n",
    "    if exp == 0:\n",
    "        exp = -126\n",
    "    else:\n",
    "        exp -= 127\n",
    "    \n",
    "    # convert the fractional part to a float\n",
    "    frac = float(frac) / (1 << 23)\n",
    "    \n",
    "    # apply the sign, exponent, and fractional part to get the final representation\n",
    "    print(\"sign: \", sign)\n",
    "    print(\"exp: \", exp)\n",
    "    print(\"frac: \", frac)\n",
    "    assert (-1)**sign * (1 + frac) * 2**exp == num\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
